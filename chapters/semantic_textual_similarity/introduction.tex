\chapter{\label{cha:sts_introduction}Introduction}


\section{What is Semantic Textual Similarity?}

\section{Related Work}

\section{Datasets}
We experimented with several datasets throughout the experiments in the Semantic Textual Similarity Section. In order to maintain the versatility of our methods we experimented with several English datasets as well as several non English datasets and several datasets from different domains which we will introduce in this section. All of the datasets which are described here re publicly available and can be considered as STS benchmarks. 

\subsection{English Datasets}
\begin{enumerate}
  \item \textbf{SICK dataset} \footnote{The SICK dataset is available to download at \url{https://wiki.cimec.unitn.it/tiki-index.php?page=CLIC}} - The SICK data contains 9927 sentence pairs with a 5,000/4,927 training/test split which were employed in the SemEval 2014 Task1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment \cite{marelli-etal-2014-semeval}. The dataset has two types of annotations: Semantic Relatedness and Textual Entailment. We only use Semantic Relatedness annotations in our research. SICK was built starting from two existing datasets: the 8K ImageFlickr data set \footnote{The 8K ImageFlickr data set is available at \url{http://hockenmaier.cs.illinois.edu/8k-pictures.html}} \cite{rashtchian-etal-2010-collecting} and the SemEval-2012 STS MSR-Video Descriptions dataset \footnote{The SemEval-2012 STS MSR-Video Descriptions dataset is available at \url{https://www.cs.york.ac.uk/semeval-2012/task6/index.html}} \cite{agirre-etal-2012-semeval}. The 8K ImageFlickr dataset is a dataset of images, where each image is associated with five descriptions. To derive SICK sentence pairs the organisers randomly selected 750 images and sampled two descriptions from each of them. The SemEval2012 STS MSR-Video Descriptions data set is a collection of sentence pairs sampled from the short video snippets which compose the Microsoft Research Video Description Corpus \footnote{The Microsoft Research Video Description Corpus is available to download at \url{https://research.microsoft.com/en-us/downloads/38cf15fd-b8df-477e-a4e4-a4680caa75af/}}. A subset of 750 sentence pairs have been randomly chosen from this data set to be used in SICK. 
  
  In order to generate SICK data from the 1,500 sentence pairs taken from the source data sets, a 3-step process has been applied to each sentence composing the pair, namely \textit{(i) normalisation, (ii) expansion and (iii) pairing} \cite{marelli-etal-2014-semeval}. The \textit{normalisation} step has been carried out on the original sentences to exclude or simplify instances that contained lexical, syntactic or semantic phenomena such as named entities, dates, numbers, multiword expressions etc. In the \textit{expansion} step syntactic and lexical transformations with predictable effects have been applied to each normalized sentence, in
  order to obtain \textit{(i)} a sentence with a similar meaning, \textit{(ii)} a sentence with a logically contradictory or at least highly contrasting meaning, and \textit{(iii)} a sentence that contains most of the same lexical items, but has a different meaning. Finally, in the \textit{pairing} step each normalised sentence in the pair has been combined with all the sentences resulting from the expansion phase and with the other normalised sentence in the pair. Furthermore, a number of pairs composed of completely unrelated sentences have been added to the data set by randomly taking two sentences from two different pairs \cite{marelli-etal-2014-semeval}. 
  
  Each pair in the SICK dataset has been annotated to mark the degree to which the two sentence meanings are related (on a 5-point scale). The ratings 
  have been collected through a large crowdsourcing study, where each pair 
  has been evaluated by 10 different annotators. Once all the annotations were collected, the relatedness gold score has been computed for each pair as the average of the ten ratings assigned by the annotators \cite{marelli-etal-2014-semeval}. Table \ref{tab:sickdata} shows examples of sentence pairs with different degrees of semantic relatedness; gold relatedness scores are expressed on a 5-point rating scale. Given a test sentence pair the machine learning models require to predict a value between 0-5 which reflects the relatedness of the given sentence pair. 
  
  \begin{table}[ht!]
  	\centering 	
  	\begin{tabular}{l|c} 
  		\hline
  		\multicolumn{1}{c|}{\textbf{Sentence Pair}} & 
  		\multicolumn{1}{c}{\textbf{Relatedness}}  \\
  		\hline
  		\makecell[l]
  		{1. A little girl is looking at a woman in costume. \\ 
  		 2. A young girl is looking at a woman in costume.} & 4.7  \\
  		\hline
  			\makecell[l]
  		{1. Nobody is pouring ingredients into a pot. \\ 
  			2. Someone is pouring ingredients into a pot. } & 3.5  \\
  		\hline
  		\makecell[l]
  		{1. Someone is pouring ingredients into a pot. \\ 
  		 2. A man is removing vegetables from a pot. } & 2.8  \\
  		\hline
  		\makecell[l]
  		{1. A man is jumping into an empty pool. \\ 
  		 2. There is no biker jumping in the air. } & 1.6  \\
  		\hline               
  	\end{tabular}
  	\caption[Example sentence pairs from the SICK dataset]{Example sentence pairs from the SICK dataset with their gold relatedness scores (on a 5-point rating scale).}
  	\label{tab:sickdata}
  \end{table}

 \item \textbf{STS 2017 English Dataset} \footnote{The STS 2017 English Dataset is available to download at \url{http://ixa2.si.ehu.es/stswiki/}} STS 2017 English Dataset was employed in SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation which is the most recent STS task in SemEval \cite{cer-etal-2017-semeval}. As the training data for the competition, participants were encouraged to make use of all existing data sets from prior STS evaluations including all previously released trial, training and evaluation data from SemEval 2012 - 2016 \cite{agirre-etal-2012-semeval,agirre-etal-2013-sem,agirre-etal-2014-semeval,agirre-etal-2015-semeval,agirre-etal-2016-semeval}. Once combined we had 8277 sentence pairs for training. More information about the datasets used to build the training set is available in Table \ref{tab:englishdata_info}.
 
 On the other hand, a fresh test set of 250 sentence pairs was provided by SemEval-2017 STS Task organisers \cite{cer-etal-2017-semeval}. The Stanford Natural Language Inference (SNLI) corpus \cite{bowman-etal-2015-large} was the primary data source for this test set. Similar to the SICK dataset, Each pair in the STS 2017 English Test set has been annotated to mark the degree to which the two sentence meanings are related (on a 5-point scale). The ratings have been collected through crowdsourcing on Amazon Mechanical Turk\footnote{Amazon Mechanical Turk is a crowdsourcing website for businesses to hire remotely located \textit{crowd workers} to perform discrete on-demand tasks. It is available at \url{https://www.mturk.com/}}. Five annotations have been collected per pair and gold score has been computed for each pair as the average of the five ratings assigned by the annotators. However, unlike the SICK dataset, the organisers has a clear explanations for the score ranges. Table \ref{tab:sts2017data} shows some example sentence pairs from the dataset with the gold labels and their explanations. Similar to the SICK dataset, the machine learning models require to predict a value between 0-5 which reflects the similarity of the given sentence pair.
 
 \begin{table}[ht!]
 	\centering
 	\begin{tabular}{c|c|c|l}
 		\hline
 		\multicolumn{1}{c|}{\textbf{Year}} & 
 		\multicolumn{1}{c|}{\textbf{Dataset}} & 
 		\multicolumn{1}{c|}{\textbf{Pairs}} & 
 		\multicolumn{1}{c}{\textbf{Source}} \\
 		\hline
 		2012 \cite{agirre-etal-2012-semeval} & MSRpar & 1500 & newswire \\
 		2012 \cite{agirre-etal-2012-semeval} & MSRvid & 1500 & videos \\
 		2012 \cite{agirre-etal-2012-semeval} & OnWN & 750 & glosses \\
 		2012 \cite{agirre-etal-2012-semeval} & SMTnews & 750 & WMT eval. \\
 		2012 \cite{agirre-etal-2012-semeval} & SMTeuroparl & 750 & WMT eval. \\
 		\hline
 		2013 \cite{agirre-etal-2013-sem} & HDL & 750 & newswire \\
 		2013 \cite{agirre-etal-2013-sem} & FNWN & 189 & glosses \\
 		2013 \cite{agirre-etal-2013-sem} & OnWN & 561 & glosses \\
 		2013 \cite{agirre-etal-2013-sem} & SMT & 750 & MT eval. \\
 		\hline
 		2014 \cite{agirre-etal-2014-semeval} & HDL & 750 & newswire headlines \\
 		2014 \cite{agirre-etal-2014-semeval} & OnWN & 750 & glosses \\
 		2014 \cite{agirre-etal-2014-semeval} & Deft-forum & 450 & forum posts \\
 		2014 \cite{agirre-etal-2014-semeval} & Deft-news & 300 & news summary \\
 		2014 \cite{agirre-etal-2014-semeval} & Images & 750 & image descriptions \\
 		2014 \cite{agirre-etal-2014-semeval} & Tweet-news & 750 & tweet-news pairs \\
 		\hline
 		2015 \cite{agirre-etal-2015-semeval} & HDL & 750 & newswire headlines \\
 		2015 \cite{agirre-etal-2015-semeval} & Images & 750 & image descriptions \\
 		2015 \cite{agirre-etal-2015-semeval} & Ans.-student & 750 & student answers \\
 		2015 \cite{agirre-etal-2015-semeval} & Ans.-forum & 375 & Q\&A forum answers \\
 		2015 \cite{agirre-etal-2015-semeval} & Belief & 375 & committed belief \\
 		\hline
 		2016 \cite{agirre-etal-2016-semeval} & HDL & 249 & newswire headlines \\
 		2016 \cite{agirre-etal-2016-semeval} & Plagiarism & 230 & short-answer plag. \\
 		2016 \cite{agirre-etal-2016-semeval} & post-editing & 244 & MT postedits \\
 		2016 \cite{agirre-etal-2016-semeval} & Ans.-Ans. & 254 & Q\&A forum answers \\
 		2016 \cite{agirre-etal-2016-semeval} & Quest.-Quest. & 209 & Q\&A forum questions \\
 		\hline
 		2017 \cite{cer-etal-2017-semeval} & Trial & 23 & Mixed STS 2016 \\
 		\hline
 	\end{tabular}
 	\caption[Information about English STS 2017 training set]{Information about the datasets used to build the English STS 2017 training set. The \textbf{Year} column shows the year of the SemEval competition that the dataset got released. \textbf{Dataset} column expresses the acronym used describe a dataset in that year. \textbf{Pairs} is the number of sentence pairs in that particular dataset and \textbf{Source} shows the source of the sentence pairs. }
 	\label{tab:englishdata_info}
 \end{table}

 
   \begin{table}[ht!]
 	\centering 	
 	\begin{tabular}{l|c} 
 		\hline
 		\multicolumn{1}{c|}{\textbf{Sentence Pair}} & 
 		\multicolumn{1}{c}{\textbf{Relatedness}}  \\
 		\hline
 		\makecell[l]
 		{\textit{The two sentences are completely equivalent}  \\ \textit{as they mean the same thing.} \\
 			1. The bird is bathing in the sink. \\ 
 			2. Birdie is washing itself in the water basin.} & 5  \\
 		\hline
 		\makecell[l]
 		{\textit{The two sentences are completely equivalent}  \\ \textit{as they mean the same thing.} \\
 			1. The bird is bathing in the sink. \\ 
 			2. Birdie is washing itself in the water basin.} & 4  \\
 		\hline
 		\makecell[l]
 		{\textit{The two sentences are roughly equivalent, but }  \\ \textit{some important information differs/missing.} \\
 			1. John said he is considered a witness but not \\ a suspect. \\ 
 			2. “He is not a suspect anymore.” John said.} & 3  \\
 		\hline
 		\makecell[l]
 		{\textit{The two sentences are not equivalent, but share } \\
 			\textit{some details.} \\
 			1. They flew out of the nest in groups. \\ 
 			2. They flew into the nest together.} & 2  \\
 		\hline
 		\makecell[l]
 		{\textit{The two sentences are not equivalent, but are } \\
 			\textit{on the same topic.} \\
 			1. The woman is playing the violin. \\ 
 			2. The young lady enjoys listening to the guitar.} & 1  \\
 		\hline
 		\makecell[l]
 		{\textit{The two sentences are completely dissimilar} \\
 			1. The black dog is running through the snow. \\ 
 			2. A race car driver is driving his car through \\ the mud.} & 0  \\
 		\hline
 	
 	\end{tabular}
 	\caption[Example sentence pairs from the STS2017 English dataset]{Example sentence pairs from the STS2017 English dataset with their gold relatedness scores (on a 5-point rating scale) and explanations.}
 	\label{tab:sts2017data}
 \end{table} 
  
 \item \textbf{Quora Question Pairs} \footnote{The Quora Question Pairs Dataset is available to download at \url{http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv}} The Quora Question Pairs dataset is a big dataset which was first released for a Kaggle Competition\footnote{Kaggle is an online community of data scientists and machine learning practitioners that hosts machine learning competitions. The Quora Question Pairs competition is available on \url{https://www.kaggle.com/c/quora-question-pairs}}. Quora is a question-and-answer website where questions are asked, answered, followed, and edited by internet users, either factually or in the form of opinions. If a particular new question has been asked before, users merge the new question to the original question flagging it as a duplicate. The organisers used this functionality to create the dataset and did not use a separate annotation process. Their original sampling method has returned an imbalanced dataset with many more true examples of duplicate pairs than non-duplicates. Therefore, the organisers have supplemented the dataset with negative examples. One source of negative examples 
 have been pairs of \textit{related question} which, although pertaining to similar topics, are not truly semantically equivalent. 
 
 The dataset has 400,000 question pairs and we used 4:1 split on that to separate it into a training set and a test set resulting 320,000 questions pairs in the training set and  80,000 sentence pairs in the testing set. The machine learning models need to predict a value between 0 and 1 that reflects whether it is a duplicate question pair or not. 1 indicates that a certain question pair is a duplicate and 0 indicates it is not a duplicate. 
 
    \begin{table}[ht!]
 	\centering 	
 	\begin{tabular}{l|c} 
 		\hline
 		\multicolumn{1}{c|}{\textbf{Question Pair}} & 
 		\multicolumn{1}{c}{\textbf{\detokenize{is-duplicate}}}  \\
 		\hline
 		\makecell[l]
 		{	1. What are natural numbers? \\ 
 			2. What is a least natural number?} & 0  \\
 		\hline
 		\makecell[l]
 		{	1. Which Pizzas are most popularly ordered \\ in Dominos menu? \\ 
 			2. How many calories does a Dominos Pizza have?} & 0  \\
 		\hline
 		\makecell[l]
 		{   1. How do you start a bakery? \\ 
 			2. How can one start a bakery business?} & 1  \\
 		\hline
 		\makecell[l]
 		{	1. Should I learn Python or Java first? \\ 
 			2. If I had to choose between learning \\ Java and Python what should I choose \\ to learn first?} & 1  \\
 		\hline
 		
 	\end{tabular}
 	\caption[Example question pairs from the Quora Question Pairs dataset]{Example question  pairs from the Quora Question Pairs dataset with their gold \detokenize{is-duplicate} value. \textbf{Question Pair} column shows the two questions and \textbf{is-duplicated} column denotes whether it is a duplicated pair or not.}
 	\label{tab:quoradata}
 \end{table}  
 
  
This is different to the previous datasets since it is not artificially created and use day to day language. Since it has more than 300,000 training instances deep learning systems will benefit more when used on this dataset. 
\end{enumerate}

\subsection{Datasets on Other Languages}
In order to evaluate the potability of our STS methods we used several non-English datasets through out the experiments which we describe below.  
\begin{enumerate}
\item{ \textbf{Spanish STS Dataset \footnote{The Spanish STS dataset can be downloaded at \url{http://alt.qcri.org/semeval2017/task1/index.php?id=data-and-tools}}}} - Spanish STS dataset that we used was employed for Spanish STS subtask in SemEval 2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation \cite{cer-etal-2017-semeval}. The training set has 1250 sentence pairs annotated with a relatedness score between 0 and 4. The training set combined several datasets from previous SemEval STS shared tasks also\cite{cer-etal-2017-semeval}. Table \ref{tab:spanishdata_info} shows more information about the trainin set. There were two sources for test set - Spanish news and Spanish Wikipedia dump having 500 and 250 sentence pairs respectively \cite{cer-etal-2017-semeval}. Both datasets were annotated with a relatedness score between 0 and 4. Table \ref{tab:spanishdata} shows few pairs of sentences with their similarity score.

\begin{table}[ht!]
	\centering
	\begin{tabular}{c|c|c|l}
		\hline
		\multicolumn{1}{c|}{\textbf{Year}} & 
		\multicolumn{1}{c|}{\textbf{Dataset}} & 
		\multicolumn{1}{c|}{\textbf{Pairs}} & 
		\multicolumn{1}{c}{\textbf{Source}} \\
		\hline
		2014 \cite{agirre-etal-2014-semeval} & Trial & 56 & NR \\
		2014 \cite{agirre-etal-2014-semeval} & Wiki  & 324 & Spanish Wikipedia \\
		2014 \cite{agirre-etal-2014-semeval} & News  & 480 & Newswire \\
		2015 \cite{agirre-etal-2014-semeval} & Wiki & 251 & Spanish Wikipedia \\
		2015 \cite{agirre-etal-2015-semeval} & News & 500 & Sewswire \\
		2017 \cite{cer-etal-2017-semeval} & Trial & 23 & Mixed STS 2016 \\
		\hline
	\end{tabular}
	\caption[Information about Spanish STS training set]{Information about the datasets used to build the Spanish STS training set. The \textbf{Year} column shows the year of the SemEval competition that the dataset got released. \textbf{Dataset} column expresses the acronym used describe a dataset in that year. \textbf{Pairs} is the number of sentence pairs in that particular dataset and \textbf{Source} shows the source of the sentence pairs. }
	\label{tab:spanishdata_info}
\end{table}


\begin{table}[ht!]
	\centering
		\begin{tabular}{l|c}
			\hline
			\multicolumn{1}{c|}{\textbf{Sentence Pair}} & 
			\multicolumn{1}{c}{\textbf{Similarity}}  \\
			\hline
			\makecell[l]{1. Amás, los misioneros apunten que los númberos \\ d'infectaos puen ser shasta dos o hasta cuatro veces \\ más grandess que los oficiales. \\
				\textit{(Furthermore, missionaries point out that the numbers of } \\ \textit{infected can be up to two or up to four times larger than} \\ \textit{the official ones.)} \\ 
				2. Los cadáveres de personas fallecidas pueden ser hasta \\ diez veces más contagiosos que los infectados vivos. \\ 
			\textit{(The corpses of deceased people can be up to ten times } \\ \textit{more contagious than those infected alive.)}} & 0.6  \\
%			\hline
%			\makecell[l]{1. Desde Colombia, el presidente Juan Manuel Santos dijo \\ que conversó por teléfono con Humala sobre el tema y que \\ entregaría al detenido a las autoridades peruanas a más tardar \\ el viernes. \\ 
%				\textit{(From Colombia, President Juan Manuel Santos said he spoke} \\ \textit{by phone with Humala about the matter and that he would hand} \\ \textit{over the detainee to the Peruvian authorities no later than Friday.)}\\
%				2. El presidente de Colombia, Juan Manuel Santos, había \\ anunciado horas antes que Orellana, que se encuentra \\ detenido, será entregado a las autoridades peruanas sentre \\ hoy y mañanas. \\
%			   \textit{(The president of Colombia, Juan Manuel Santos, had announced} \\ \textit{hours before that Orellana, who is in detention, will be handed over} \\ \textit{to the Peruvian authorities today and tomorrow.)}}  
%			 & 3.2 \\
			\hline
			\makecell[l]{1. La policía abatió a un caníbal cuando devoraba a una  \\ mujer Matthew Williams, de 34 años, fue sorprendido en \\ la madrugada mordiendo el rostro de una joven a la que  \\ había invitado a su hotel. \\ 
				\textit{(Police killed a cannibal while devouring a woman Matthew } \\ \textit{Williams, 34, was caught early in the morning biting the} \\ \textit{ face of a young woman he had invited to his hotel.)} \\
				2. La policía de Gales del Sur mató a un caníbal cuando se \\ estaba comiendo la cara de una mujer de 22 años en la \\ habitación de un hotel. \\ 
			\textit{(South Wales police killed a cannibal when he was eating the } \\ \textit{face of a 22-year-old woman in a hotel room.)} } & 2  \\
			\hline
			\makecell[l]{1. Ollanta Humala se reúne mañana con el Papa Francisco. \\
				\textit{(Ollanta Humala meets tomorrow with Pope Francis.)} \\ 
				2. El Papa Francisco mantuvo hoy una audiencia privada \\ con el presidente Ollanta Humala, en el Vaticano. \\
			\textit{(Pope Francis held a private audience today with President} \\ \textit{Ollanta Humala, at the Vatican.)}} & 3  \\
			\hline               
		\end{tabular}
	\caption[Example sentence pairs from the Spanish STS dataset]{Example sentence pairs from the Spanish STS dataset. \textbf{Sentence Pair} column shows the two sentences. We also included their translations in the table. The translations were done by a native Spanish speaker. \textbf{Similarity} column indicates the annotated similarity of the two sentences. }
	\label{tab:spanishdata}
\end{table}  
	
\item{ \textbf{Arabic STS Dataset \footnote{The Arabic STS dataset can be downloaded at \url{http://alt.qcri.org/semeval2017/task1/index.php?id=data-and-tools}}}} The Arabic STS dataset we selected was also used for the Arabic STS subtask in SemEval 2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation \cite{cer-etal-2017-semeval}. Unlike Spanish, no data from previous SemEval competitions were available since this was the first time an Arabic STS task was organised in SemEval. More information about the extracted sentences will be shown in the Table \ref{tab:arabicdata_info}. 

\begin{table}[ht!]
	\centering
	\begin{tabular}{c|c|l}
		\hline
		\multicolumn{1}{c|}{\textbf{Dataset}} & 
		\multicolumn{1}{c|}{\textbf{Pairs}} & 
		\multicolumn{1}{c}{\textbf{Source}} \\
		\hline
		Trial & 23 & Mixed STS 2016 \\
		MSRpar  & 510 & newswire \\
		MSRvid  & 368 & videos \\
		SMTeuroparl  & 203 & WMT eval. \\
		\hline
	\end{tabular}
	\caption[Information about Arabic STS training set]{Information about the datasets used to build the Arabic STS training set. \textbf{Dataset} column expresses the acronym used describe the dataset. \textbf{Pairs} is the number of sentence pairs in that particular dataset and \textbf{Source} shows the source of the sentence pairs. }
	\label{tab:arabicdata_info}
\end{table}  

To prepare the annotated instances, a subset of the English STS 2017 dataset has been selected and human translated into Arabic. Sentences have been translated independently from their pairs. Arabic translation has been provided by native Arabic speakers with strong English skills in Carnegie Mellon University in Qatar . Translators have been given an English sentence and its Arabic machine translation5 where they have performed post-editing to correct errors.  STS labels have been then transferred to the translated pairs. Therefore, annotation guidelines and the template will be similar to the English STS 2017 dataset. 1103 sentence pairs were available for training and 250 sentence pairs were available in the test set. Table \ref{tab:arabicdata} shows few pairs of sentences with their similarity score.  

    \begin{table}[ht!]
	\centering 	
	\begin{tabular}{l|c} 
		\hline
		\multicolumn{1}{c|}{\textbf{Sentence Pair}} & 
		\multicolumn{1}{c}{\textbf{\detokenize{Similarity}}}  \\
		\hline
		\makecell[l]
		{	1. \< أحدهم يقلي لحما.  > \\ \textit{Someone is frying meat.}\ \\ 
			2.  \< أحدهم يعزف البيانو.  > \\ \  \textit{Someone plays the piano.} } & 0.250
		\\
		\hline
		\makecell[l]
	{	1. \< أمرأة تظيف المكونات في الإناء.  > \\ \textit{
			A woman cleaning ingredients in the bowl.}\ \\ 
		2.  \< إمرأة تكسر ثلاثة بيضات في الإناء. > \\ \  \textit{
			A woman breaks three eggs in a bowl.} } & 1.750
	\\
	\hline
			\makecell[l]
		{	1. \< طفلة تعزف القيثارة.  > \\ \textit{A Child is playing harp.}\ \\ 
			2.  \< رجل يعزف القيثارة . > \\ \  \textit{A man plays the harp.} } & 2.250
		\\
		\hline
			\makecell[l]
		{	1. \< المرأة تقطع البصل الأخضر.  > \\ \textit{The woman chops green onions.}\ \\ 
			2.  \< إمرأة تقشر بصلة. > \\ \  \textit{A woman peeling an onion.} } & 3.250
		\\
		\hline
		\makecell[l]
		{	1. \< رجل يرقص على صقف الغرفة.  > \\ \textit{A man dancing on the roof of the room.}\ \\ 
			2.  \< رجل يرقص رأسا على عقب على السقف. > \\ \  \textit{A man is dancing upside down on the ceiling.} } & 4.600
		\\
		\hline
		
	\end{tabular}
	\caption[Example question pairs from the Arabic STS dataset]{Example question  pairs from the Arabic STS dataset. \textbf{Sentence Pair} column shows the two sentences. We also included their translations in the table. The translations were done by a native Arabic speaker. \textbf{Similarity} column indicates the annotated similarity of the two sentences.}
	\label{tab:arabicdata}
\end{table}  



	
\end{enumerate}


\subsection{Datasets on Different Domains}

\begin{enumerate}
	\item{ \textbf{Bio-medical STS Dataset}} - 
	\cite{10.1093/bioinformatics/btx238}
	
	\item{ \textbf{Clinical STS Dataset}}
	\cite{Wang2020}	
\end{enumerate}

\section{Applications}

%%% The following are used by emacs, and similar:

%%% Local Variables: ***
%%% TeX-master: "../thesis.tex"  ***
%%% End: ***
