\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{asparagus}{rgb}{0.53, 0.66, 0.42}

\DeclareRobustCommand{\hlgreen}[1]{{\sethlcolor{asparagus}\hl{#1}}}

\DeclareRobustCommand{\hlred}[1]{{\sethlcolor{brickred}\hl{#1}}}

\newcommand{\hlc}[2][yellow]{{%
		\colorlet{foo}{#1}%
		\sethlcolor{foo}\hl{#2}}%
}


\chapter{\label{cha:qe_introduction}Introduction to Translation Quality Estimation}

The goal of quality estimation (QE) is to evaluate the quality of a translation without having access to a reference translation \autocite{specia-etal-2018-findings}. High-accuracy QE that can be easily deployed for a number of language pairs is the missing piece in many commercial translation workflows as they have numerous potential uses. They can be employed to select the best translation when several translation engines are available or can inform the end-user about the reliability of automatically translated content. In addition, QE systems can be used to decide whether a translation can be published as it is in a given context, or whether it requires human post-editing before publishing or even translation from scratch by a human  \autocite{kepler-etal-2019-openkiwi}. 

Quality estimation task is different from automatic machine translation evaluation \autocite{barrault-etal-2020-findings}. Automatic machine translation evaluation approaches such as BLEU \autocite{papineni-etal-2002-bleu} need the reference translation in order to perform the machine translation evaluation. As mentioned before, quality estimation, on the other hand, does not need the reference translation. Therefore, automatic MT evaluation approaches such as BLEU \autocite{papineni-etal-2002-bleu}, METEOR \autocite{banerjee-lavie-2005-meteor}, LEPOR \autocite{han-etal-2012-lepor} and others can not be directly applied in the QE task. As a result, the solutions proposed for QE are completely different from that of automatic machine translation evaluation.

The estimation of translation quality can be done at different levels: word/phrase-level, sentence-level and, document-level \autocite{ive-etal-2018-deepquest}. Word-level QE aims to spot words that need to be reviewed during the post-editing process. It indicates which words from the source have been incorrectly translated in the target and whether the words inserted between these words are correct. Sentence-level QE models provide a single score for each pair of source and target sentences. Sentence-level QE scores help to rank translations that are worth post-editing. Document-level QE, on the other hand, scores or ranks documents according to their quality for fully automated MT usage scenarios \autocite{ive-etal-2018-deepquest}. In recent years, word-level QE and sentence-level QE have been more popular among the community \autocite{specia-etal-2018-findings}.

During the past decade, there has been tremendous progress in the field of quality estimation, largely as a result of the QE shared tasks organised annually by the Workshops on Statistical Machine Translation (WMT), more recently called as the Conferences on Machine Translation, since 2012  \autocite{callison-burch-etal-2012-findings,bojar-etal-2013-findings,bojar-etal-2014-findings,bojar-etal-2015-findings,bojar-etal-2016-findings,bojar-etal-2017-findings,specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}. Firstly, they have provided annotated datasets which can be used to train QE models and to evaluate them. Secondly, the annotated datasets these shared tasks released each year have led to the development of many open-source QE systems \autocite{specia-etal-2015-multi, ive-etal-2018-deepquest, kepler-etal-2019-openkiwi}. 

At present neural-based QE methods constitute state-of-the-art in quality estimation. However, these approaches are based on complex neural networks and require resource-intensive training. This resource-intensive nature of these deep learning based frameworks makes it expensive to train QE models. Furthermore, these architectures require a large number of annotated instances for training, making the quality estimation task very difficult for the low-resource language pairs. This nature of the current state-of-the-art QE systems has hindered their popularity in real-world applications.

This part of the thesis addresses these problems by employing simple STS architectures we experimented with in Part I of the thesis in quality estimation. We redefine the QE task as a cross-lingual STS task and show that state-of-the-art STS architectures can be applied in the QE task by changing the input embeddings. As far as we know, this is the first study done on applying neural STS models directly to the QE task. 

In Chapter \ref{cha:qe_transquest}, we explore sentence-level QE with STS architectures. We evaluate their performance in recent sentence-level QE datasets, comparing them with open-source QE tools such as OpenKiwi \autocite{kepler-etal-2019-openkiwi} and QuEst++ \autocite{specia-etal-2015-multi}. Finally, we propose a new state-of-the-art QE method for sentence-level QE.

In Chapter \ref{cha:qe_word}, we expand this idea to word-level QE. We modify the output of the STS architecture to predict word-level translation qualities. We evaluated their performance in recent word-level QE datasets, comparing the results with open-source word-level QE tools such as OpenKiwi \autocite{kepler-etal-2019-openkiwi} and Marmot \autocite{logacheva-etal-2016-marmot}. We show that in most of the datasets, our simple word-level architecture outperforms other QE tools. 

In Chapter \ref{cha:qe_multilingual}, for the first time, we explore multilingual QE with state-of-the-art word-level and sentence-level QE methods. Furthermore, we evaluate multilingual QE in different training environments, including zero-shot and few-shot. Our findings in Chapter \ref{cha:qe_multilingual} would be beneficial for low resource languages. 


The main contributions of this part of the thesis are as follows.

\begin{enumerate}

	\item We propose two STS architectures based on transformers to perform sentence-level QE. These architectures are simpler than the  architectures available in OpenKiwi \autocite{kepler-etal-2019-openkiwi} and DeepQuest \autocite{ive-etal-2018-deepquest}. We evaluate them on 15 language pairs in which sentence-level QE data was available, and we show that the two architectures outperform the current state-of-the-art sentence-level QE frameworks such as OpenKiwi \autocite{kepler-etal-2019-openkiwi} and DeepQuest \autocite{ive-etal-2018-deepquest}.
	
	
	\item We introduce a simple architecture to perform word-level quality estimation that predicts the quality of the words in the source sentence, target sentence and the gaps in the target sentence. We evaluate it on eight different language pairs in which the word-level QE data was available, and we show that the proposed architecture outperforms the current state-of-the-art word-level QE frameworks like Marmot \autocite{logacheva-etal-2016-marmot} and OpenKiwi \autocite{kepler-etal-2019-openkiwi}.

	\item We propose multilingual learning for QE with the proposed architectures for sentence-level and word-level. We show that multilingual models are helpful in low-resource languages where the training data is difficult to find. 
	
	\item We provide important resources to the community. The code of each chapter is bundled to an open-source QE framework, and the pre-trained sentence-level and word-level QE models will be freely available to the community. The link to the relevant code and the models will be unveiled in the introduction section of each chapter. 
\end{enumerate}

The remainder of this chapter is structured as follows. Section \ref{sec:qe_related} explores the various methods that have been employed in sentence-level and word-level QE, including previous research done incorporating STS in the QE task. Section \ref{sec:qe_datasets} discuss the various datasets we used in this part of the thesis. In Section \ref{sec:qe_evaluation}, we show the main evaluation metrics used for the sentence-level and word-level QE experiments in the following Chapters in Part III of the thesis. The chapter finishes with the conclusions.



\section{Related Work}
\label{sec:qe_related}
Before the neural network era, most of the quality estimation systems such as QuEst \autocite{specia-etal-2013-quest} and QuEst++ \autocite{specia-etal-2015-multi} were heavily dependent on linguistic processing and feature engineering to train traditional machine-learning algorithms including support vector regression and randomised decision trees \autocite{specia-etal-2013-quest}. These features can be either extracted from the machine translation system (\textit{glass-box} features) or obtained from
the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (\textit{black-box} features) \autocite{specia-etal-2009-estimating}. For example, QuEst \autocite{specia-etal-2013-quest} has 17 manually crafted features fed into a support vector regression algorithm. These 17 features consist of glass-box features such as \textit{ratio of number of tokens in source and target segments}, \textit{ratio of percentage of nouns/verbs in the source and target} etc. as well as black-box features such as \textit{global score and relevant features of the SMT system}, \textit{proportion of pruned search graph nodes} etc. In QuEst++, the number of features varies from 80 to 123 depending on the language pair \autocite{specia-etal-2015-multi}. QuEst \autocite{specia-etal-2013-quest}, QuEst++ \autocite{specia-etal-2015-multi} and Marmot \autocite{logacheva-etal-2016-marmot} can be considered as the most popular traditional QE tools. QuEst \autocite{specia-etal-2013-quest} only supports sentence-level QE, Marmot \autocite{logacheva-etal-2016-marmot} only supports word-level QE while QuEst++ \autocite{specia-etal-2015-multi} can support both word-level and sentence-level QE. Even though, they provided good results in early days, these traditional approaches are no longer the state-of-the-art. In recent years, neural-based QE systems have consistently topped the leader boards in WMT quality estimation shared tasks  \autocite{kepler-etal-2019-openkiwi}.  

With the increasing popularity of word embeddings \autocite{DBLP:journals/corr/abs-1301-3781}, neural networks based on word embeddings got popular in the QE field. They outperformed traditional QE systems and provided state-of-the-art results. For example, the best-performing system at the WMT 2017 shared task on QE was \textsc{POSTECH}, which is purely neural and does not rely on feature engineering at all \autocite{kim-etal-2017-predictor}. \textsc{POSTECH} revolves around an encoder-decoder Recurrent Neural Network (RNN) (referred to as the `predictor'), stacked with a bidirectional RNN (the `estimator') that produces quality estimates. In the predictor, an encoder-decoder RNN model predicts words based on their context representations and in the estimator step, there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, \textsc{POSTECH} requires extensive predictor pre-training, which means it depends on large parallel data and is computationally intensive \autocite{ive-etal-2018-deepquest}. The \textsc{POSTECH} architecture was later re-implemented in DeepQuest  \autocite{ive-etal-2018-deepquest}. DeepQuest supports both word-level and sentence-level QE \autocite{ive-etal-2018-deepquest}. 

OpenKiwi \autocite{kepler-etal-2019-openkiwi} is another open-source neural QE framework developed by Unbabel. It implements four different neural network architectures \textsc{QUETCH} \autocite{kreutzer-etal-2015-quality}, \textsc{NuQE} \autocite{martins-etal-2016-unbabels}, Predictor-Estimator \autocite{kim-etal-2017-predictor} and a stacked model of these architectures. Both the \textsc{QUETCH} and \textsc{NuQE} architectures have simple neural network models that do not rely on additional parallel data but do not perform that well. The Predictor-Estimator model is similar to the \textsc{POSTECH} architecture and relies on additional parallel data. In OpenKiwi, the best performance for sentence-level quality estimation was given by the stacked model that used the Predictor-Estimator model, meaning that the best model requires extensive predictor pre-training and relies on large parallel data and computational resources. 

As discussed before, these models' complex and resource-intensive nature creates many limitations when deployed in real-world scenarios. Therefore, in this study, we propose to use simple STS architectures in QE. Over the years, there have been a few attempts to integrate semantic similarity into QE. \textcite{specia2011predicting} employed semantic information into the QE task to address the problem of meaning preservation in translation. The authors integrated semantic similarity features to the QE model and improved the results of the QE task \autocite{specia2011predicting}. \textcite{bicici-way-2014-referential} introduced the use of referential translation machines (RTM) for QE. RTM is a computational model for judging monolingual and bilingual similarity that achieves state-of-the-art results. This approach provided the best result in both sentence-level and word-level tasks in WMT 2013 \autocite{bojar-etal-2013-findings}. Furthermore, \textcite{kaljahi-etal-2014-syntax} and \textcite{camargo-de-souza-etal-2014-fbk} used syntactic and semantic information in quality estimation and were able to improve over the baseline when combining these features with the features of the baseline. Finally, in a different approach, \textcite{bechara-etal-2016-semantic} used semantically similar sentences and their quality scores as features to estimate the quality of machine translated sentences. This method improved the prediction of machine translation quality for semantically similar sentences \autocite{bechara-etal-2016-semantic}. 

Even though there are several studies done to integrate semantic similarity into QE, as far as we know, this is the first study to employ state-of-the-art neural STS models directly in the QE task. 

\section{Datasets}
\label{sec:qe_datasets}

 All the datasets that we used in this part of the thesis are publicly available and were released in WMT quality estimation tasks in recent years \autocite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}. This was done to ensure the replicability of our experiments and to allow us to compare our results with state-of-the-art methods. The following sections will describe the sentence-level and word-level QE datasets we experimented, separately.

\subsection{Sentence-level QE}
Sentence-level QE datasets that we used in this part of the thesis can be categorised into two main areas depending on the aspect they have been annotated; Human-mediated Translation Edit Rate (HTER) and Direct Assessment (DA). Most of the early datasets have been annotated on HTER. Very recently DA aspect is getting popular in the QE community. We describe each of them in detail in the following sections.


\paragraph{Predicting HTER}
The performance of sentence-level QE systems has typically been assessed using the semiautomatic HTER. HTER is an edit-distance-based measure that captures the distance between the automatic translation and a reference translation in terms of the number of modifications required to transform one into another. In light of this, a QE system should be able to predict the percentage of edits required in the translation. We used several language pairs for which HTER information was available: English-Chinese (En-Zh), English-Czech (En-Cs), English-German (En-De), English-Russian (En-Ru), English-Latvian (En-Lv) and German-English (De-En). The texts are from a variety of domains, and the translations were produced using both neural and statistical machine translation systems. More details about these datasets can be found in Table \ref{tab:hter_data} and in \autocite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}. Several examples from WMT 2020, En-De dataset is shown in Table \ref{tab:hter_data_examples}.

\begin{table}[t]
	\begin{center}
		\scalebox{0.65}{
		\begin{tabular}{ |c|c|c|c|c| } 
			\hline
			\textbf{Language Pair} & \textbf{Source} & \textbf{MT system} & \textbf{Competition} &  \textbf{train, dev, test size} \\ 
			\hline
			De-En & Pharmaceutical & Phrase-based SMT & WMT 2018 \autocite{specia-etal-2018-findings} & 25,963, 1,000, 1,000  \\
			\hline
			En-Zh & Wiki & fairseq based NMT & WMT 2020 \autocite{specia-etal-2020-findings-wmt} & 7,000, 1,000, 1,000 \\
			\hline
			En-Cs & IT & Phrase-based SMT & WMT 2018 \autocite{specia-etal-2018-findings} & 40,254, 1,000, 1,000 \\
			\hline
			En-De & Wiki & fairseq based NMT & WMT 2020 \autocite{specia-etal-2020-findings-wmt} & 7,000, 1,000, 1,000 \\
			\hline
			En-De & IT & Phrase-based SMT & WMT 2018 \autocite{specia-etal-2018-findings}  & 26,273, 1,000, 1,000 \\
			\hline
			En-Ru & Tech & Online NMT & WMT 2019 \autocite{fonseca-etal-2019-findings} & 15,089, 1,000, 1,000 \\
			\hline
			En-Lv & Pharmaceutical & Attention-based NMT & WMT 2018 \autocite{specia-etal-2018-findings} & 12,936, 1,000, 1,000 \\
			\hline
			En-Lv & Pharmaceutical & Phrase-based SMT & WMT 2018 \autocite{specia-etal-2018-findings}  & 11,251, 1,000, 1,000 \\
			\hline
		\end{tabular}
	}
	\end{center}
	\caption[Information about language pairs used to predict HTER]{Information about language pairs used to predict HTER. The \textbf{Language Pair} column shows the language pairs we used in ISO 639-1 codes\protect\footnotemark. \textbf{Source} expresses the domain of the sentence and \textbf{MT system} is the Machine Translation system used to translate the sentences. 
		In that column NMT indicates Neural Machine Translation and SMT indicates Statistical Machine Translation. 
		\textbf{Competition} shows the quality estimation competition in which the data was released and the last column indicates the number of instances the train, development and test dataset had in each language pair respectively.} 
	\label{tab:hter_data}
\end{table}

\footnotetext{Language codes are available in ISO 639-1 Registration Authority Website Online - \url{ https://www.loc.gov/standards/iso639-2/php/code_list.php}}

\begin{table}[t] 
	\centering
	\scalebox{0.95}{
		\begin{tabular}{ |p{5.5cm}|p{5.5cm}|c| }
			\hline
			\multicolumn{1}{|c|}{\textbf{Source}}  & \multicolumn{1}{c|}{\textbf{Target}}  & \textbf{HTER}  \\
			\hline
			José Ortega y Gasset visited Husserl at Freiburg in 1934 . & 1934 besuchte José Ortega y Gasset Husserl in Freiburg . & 0.3333 \\
			\hline
			however , a disappointing ninth in China meant that he dropped back to sixth in the standings . & eine enttäuschende Neunte in China bedeutete jedoch , dass er in der Gesamtwertung auf den sechsten Platz zurückfiel . & 0.2000 \\
			\hline
			" Renaissance Humanism and the Future of the Humanities . " & " Renaissance Humanism and the Future of the Humanities " . & 1.0000 \\
			\hline
		sophomore Jacory Harris directed the newly implemented offense . & Sophomore Jacory Harris leitete die neu umgesetzte Straftat . & 0.0000 \\
			\hline
			
		\end{tabular}
	}
	\caption[Examples source/target pairs from WMT 2020 En-De HTER dataset.]{Examples source/target pairs from WMT 2020 En-De HTER dataset \autocite{specia-etal-2020-findings-wmt}.  \textbf{Source} column shows the source sentence in English and \textbf{Target} shows the target sentence in German. \textbf{HTER} column shows the annotated HTER value for the translation. 
	}
	\label{tab:hter_data_examples}
\end{table}



\paragraph{Predicting DA}
Even though HTER has been typically used to assess quality in machine translations, the reliability of this metric for evaluating the performance of quality estimation systems has been questioned by researchers \autocite{graham-etal-2016-glitters}. The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality \autocite{graham_baldwin_moffat_zobel_2017}, where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics \autocite{graham-etal-2015-accurate}. 

We used a recently created dataset to predict DA in machine translations which was released for the WMT 2020 quality estimation shared task 1 \autocite{specia-etal-2020-findings-wmt}. The dataset is composed of data extracted from Wikipedia for six language pairs, consisting of high-resource English-German (En-De) and English-Chinese (En-Zh), medium-resource Romanian-English (Ro-En) and Estonian-English (Et-En), and low-resource Sinhala-English (Si-En) and Nepalese-English (Ne-En), as well as a Russian-English (En-Ru) dataset which combines articles from Wikipedia and Reddit \autocite{fomicheva-etal-2020-unsupervised}. These datasets have been collected by translating sentences sampled from source-language articles using state-of-the-art NMT models built using the fairseq toolkit \autocite{ott-etal-2019-fairseq} and annotated with DA scores by professional translators. Each translation was rated with a score from 0-100 according to the perceived translation quality by at least three translators \autocite{specia-etal-2020-findings-wmt}. The DA scores were standardised using the z-score. The quality estimation systems evaluated on these datasets have to predict the mean DA z-scores of test sentence pairs. Each language pair has 7,000 sentence pairs in the training set, 1,000 sentence pairs in the development set and another 1,000 sentence pairs in the testing set. 

 \begin{table}[t] 
 	\centering
 	\scalebox{0.95}{
 		\begin{tabular}{ |p{5.5cm}|p{5.5cm}|c| }
 			\hline
 			\multicolumn{1}{|c|}{\textbf{Source}}  & \multicolumn{1}{c|}{\textbf{Target}}  & \textbf{DA}  \\
 			\hline
 		Tratatul de Aderare fusese semnat la 16 aprilie 2003. & The Accession Treaty had been signed on 16 April 2003. & 0.6226 \\
 			\hline
 			Are o industrie turistică bine dezvoltată, fiind o destinație populară printre turiștii britanici și germani. & It has a well-developed tourist industry as a popular destination among British and German tourists. & 0.9386 \\
 			\hline
 			Numărul mare de avioane de vânătoare trimise în misiuni împotriva avioanelor de recunoaștere nu a fost o greșeală. & There was no mistake in the large number of hunting aeroplanes deployed in missions against recognition aeroplanes.	 & 0.0024 \\
 			\hline
 			Ar fi fost inutil să încerce obținerea unei o catedre universitare. & It would have been pointless to try to get a university catalogue. & 0.5456 \\
 			\hline
 			
 		\end{tabular}
 	}
 \caption[Examples source/target pairs from WMT 2020 Ro-En DA dataset.]{Examples source/target pairs from WMT 2020 Ro-En DA dataset \autocite{specia-etal-2020-findings-wmt}.  \textbf{Source} column shows the source sentence in English and \textbf{Target} shows the target sentence in German. \textbf{DA} column shows the annotated DA value for the translation. 
 }
 	\label{tab:da_data_examples}
 \end{table}

\subsection{Word-level QE}
Word-level QE annotations are not straightforward as sentence-level QE annotations. They have been annotated for words in the target (`OK' for correct words, `BAD' for incorrect words), gaps in the target (`OK' for genuine gaps, `BAD' for gaps indicating missing words) and source words (`BAD' for words that lead to errors in the target, `OK' for other words) \autocite{specia-etal-2018-findings}. To make it clearer, consider the following example from En-De in WMT 2018 \autocite{specia-etal-2018-findings}. The word-level quality estimation labels would be followed. Please note that \textit{green} represents `OK' and \textit{red} represents `BAD' labels. 


\noindent \textbf{Source} - \textit{for example , you could create a document containing a car that moves across the Stage .}

\noindent \textbf{Target} - \textit{Sie können beispielsweise ein Dokument erstellen , das ein Auto über die Bühne enthält .}


\noindent \textbf{Source} - \textit{\hlgreen{for} \hlgreen{example} \hlgreen{,} \hlgreen{you} \hlgreen{could} \hlgreen{create} \hlgreen{a} \hlgreen{document} \hlgreen{containing} \hlred{a} \hlred{car} \hlred{that} \hlred{moves} \hlgreen{across} \hlgreen{the} \hlgreen{Stage} \hlgreen{.}}



\noindent \textbf{Target} - \textit{\hlgreen{<GAP>} \hlgreen{Sie} \hlgreen{<GAP>} \hlgreen{können} \hlgreen{<GAP>} \hlgreen{beispielsweise} \hlgreen{<GAP>} \hlgreen{ein} \hlgreen{<GAP>} \hlgreen{Dokument} \hlgreen{<GAP>} \hlgreen{erstellen} \hlgreen{<GAP>} \hlgreen{,} \hlgreen{<GAP>} \hlgreen{das} \hlgreen{<GAP>} \hlgreen{ein} \hlgreen{<GAP>} \hlgreen{Auto} \hlred{<GAP>} \hlgreen{über} \hlgreen{<GAP>} \hlgreen{die} \hlgreen{<GAP>} \hlgreen{Bühne} \hlgreen{<GAP>} \hlred{enthält} \hlgreen{<GAP>} \hlgreen{.} \hlgreen{<GAP>}}


As can be seen in the example, all the words in the source, all the words in the target and all the \textit{gaps} in the target have been annotated as `OK' or `BAD'.  Most of the language pairs that are annotated for sentence-level HTER scores also have word-level quality labels. Therefore, for the word-level QE experiments, we used the same language pairs we used for sentence-level HTER, which are shown in Table \ref{tab:hter_data}. More information about the word-level annotations are available on \autocite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}.


\section{Evaluation}
\label{sec:qe_evaluation}
For the evaluation, we used the same approach proposed in the WMT shared tasks so that we can compare our results with the respective baselines and the best systems submitted in each shared task. Obviously, the sentence-level and word-level QE follow two different evaluation criteria, which we explain in the following sections. 

\paragraph{Sentence-Level QE Evaluation} : Similar to the STS evaluation in Part I of the thesis, Pearson's Correlation Coefficient ($\rho$) is the most popular evaluation metric in recent WMT sentence-level QE shared tasks \autocite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}. Since the gold labels are continuous in both HTER and DA and the models need to predict a continuous value, it makes sense to employ Pearson's Correlation Coefficient as the evaluation metric. A QE model with a Pearson's Correlation Coefficient close to 1 indicates that the predictions of that model and gold labels have a strong positive linear correlation, and therefore, it is a good model to predict sentence-level QE.  


\paragraph{Word-level QE Evaluation} : The primary evaluation metric for word-level QE is the multiplication of F1-scores for the OK and BAD classes, denoted as $F1_{\textit{MULTI}}$ \autocite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}. The standard equation for the F1 score is shown in Equation \ref{equ:f1} where TP, TN, FP and FN are True Positive, True Negative, False Positive and False Negative, respectively. This F1 score is calculated for OK and BAD classes separately, and then, they are multiplied to get the $F1_{\textit{MULTI}}$ as shown in Equation \ref{equ:f1_multi}. 

\begin{equation}
\label{equ:f1}
	F1 = \frac{2*TP}{2*TP+FP+FN}
\end{equation}


\begin{equation}
	\label{equ:f1_multi}
	F1_{\textit{MULTI}} = F1_{\textit{OK}} \times F1_{\textit{BAD}}
\end{equation}

Prior to WMT 2019, $F1_{\textit{MULTI}}$ score was calculated separately for words in the source ($F1_{\textit{MULTI}} \; Source$), words in the target ($F1_{\textit{MULTI}} \; Target$) and gaps in the target ($F1_{\textit{MULTI}} \; GAPS$) \autocite{specia-etal-2018-findings}, while after WMT 2019 \autocite{fonseca-etal-2019-findings,specia-etal-2020-findings-wmt} they produce a single result for target gaps and words named as "$F1_{\textit{MULTI}} \; Target$" alongside with "$F1_{\textit{MULTI}} \; Source$". We follow the same approach.  A QE model with a $F1_{\textit{MULTI}}$ score close to 1 indicates that the predictions of that model and gold labels are similar and therefore, it is a good model to predict word-level QE.    

\section{Conclusion}
\label{sec:qe_conclusion}
Quality estimation is an important component in making machine translation useful in real-world applications. It aims to inform the user of the quality of the MT output at test time. This process can be done on different levels such as word-level, sentence-level and document-level. QE shared tasks organised annually by WMT have increased QE's popularity among the MT community by leading the development of standard datasets covering a variety of languages and domains. These QE shared tasks have further contributed to the development of evaluation measures in QE.  We followed the same evaluation measures; Pearson's Correlation Coefficient for sentence-level and $F1_{\textit{MULTI}}$ for word-level.

The annotated datasets these shared tasks released each year have led to the development of many open-source QE systems. Similar to other NLP fields, most of the early QE approaches including QuEst \autocite{specia-etal-2013-quest}, QuEst++ \autocite{specia-etal-2015-multi} and Marmot \autocite{logacheva-etal-2016-marmot} were also based on traditional machine learning and involved heavy feature engineering. However, they no longer provide competitive results. The current state-of-the-art in QE is neural models. Following this, many open-source neural QE frameworks such as OpenKiwi \autocite{kepler-etal-2019-openkiwi} and DeepQuest \autocite{ive-etal-2018-deepquest} have been created. However, these neural QE architectures are complex and need a lot of computing resources to train a QE model, which we have identified as a major limitation. To address this weakness, we propose to redefine the QE task as a cross-lingual STS task and apply the STS architectures we experimented in Part I of the thesis to QE, which are considerably simpler than the current state-of-the-art QE models.

In the next few chapters, we will be exploring the STS architectures in the QE task. First, in Chapter \ref{cha:qe_transquest}, the STS architectures will be applied in sentence-level QE, and then in Chapter \ref{cha:qe_word}, these architectures will be extended to word-level QE. Finally, in Chapter \ref{cha:qe_multilingual}, we explore multilingual QE with the proposed architectures.
