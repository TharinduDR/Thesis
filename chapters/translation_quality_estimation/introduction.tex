\chapter{\label{cha:qe_introduction}Introduction}

\section{What is Translation Quality Estimation?}

\section{Datasets}
\label{sec:datasets}

We used the architectures described above to predict two standard measures that express the quality of a translation: Human-mediated Translation Edit Rate (HTER) and Direct Assessment (DA).  All the datasets that we used are publicly available and were released in WMT quality estimation tasks in recent years \cite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}. This was done to ensure replicability of our experiments and to allow us to compare our results with the state of the art. In the reminder of the section we provide more details about the datasets used. 

\subsection{Predicting HTER}
The performance of QE systems has typically been assessed using the semiautomatic HTER (Human-mediated Translation Edit Rate). HTER is an edit-distance-based measure which captures the distance between the automatic translation and a reference translation in terms of the number of modifications required to transform one into another. In light of this, a QE system should be able to predict the percentage of edits required in the translation. We used several language pairs for which HTER information was available: English-Chinese (En-Zh), English-Czech (En-Cs), English-German (En-De), English-Russian (En-Ru), English-Latvian (En-Lv) and German-English (De-En). The texts are from a variety of domains and the translations were produced using both neural and statistical machine translation systems. More details about these datasets can be found in Table \ref{tab:hter_data} and in \cite{specia-etal-2018-findings,fonseca-etal-2019-findings,specia-etal-2020-findings-wmt}.

\begin{table}[t]
	\begin{center}
		\scalebox{0.75}{
		\begin{tabular}{ |c|c|c|c|c| } 
			\hline
			\textbf{Language Pair} & \textbf{Source} & \textbf{MT system} & \textbf{Competition} &  \textbf{train, dev, test size} \\ 
			\hline
			De-En & Pharmaceutical & Phrase-based SMT & WMT 2018 \cite{specia-etal-2018-findings} & 25,963, 1,000, 1,000  \\
			\hline
			En-Zh & Wiki & fairseq based NMT & WMT 2020 \cite{specia-etal-2020-findings-wmt} & 7,000, 1,000, 1,000 \\
			\hline
			En-Cs & IT & Phrase-based SMT & WMT 2018 \cite{specia-etal-2018-findings} & 40,254, 1,000, 1,000 \\
			\hline
			En-De & IT & fairseq based NMT & WMT 2019 \cite{fonseca-etal-2019-findings} & 13,442, 1,000, 1,000 \\
			\hline
			En-De & IT & Phrase-based SMT & WMT 2018 \cite{specia-etal-2018-findings}  & 26,273, 1,000, 1,000 \\
			\hline
			En-Ru & Tech & Online NMT & WMT 2019 \cite{fonseca-etal-2019-findings} & 15,089, 1,000, 1,000 \\
			\hline
			En-Lv & Pharmaceutical & Attention-based NMT & WMT 2018 \cite{specia-etal-2018-findings} & 12,936, 1,000, 1,000 \\
			\hline
			En-Lv & Pharmaceutical & Phrase-based SMT & WMT 2018 \cite{specia-etal-2018-findings}  & 11,251, 1,000, 1,000 \\
			\hline
		\end{tabular}
	}
	\end{center}
	\caption{Information about language pairs used to predict HTER. The \textbf{Language Pair} column shows the language pairs we used in ISO 639-1 codes\protect\footnotemark. \textbf{Source} expresses the domain of the sentence and \textbf{MT system} is the Machine Translation system used to translate the sentences. 
		In that column NMT indicates Neural Machine Translation and SMT indicates Statistical Machine Translation. 
		\textbf{Competition} shows the quality estimation competition in which the data was released and the last column indicates the number of instances the train, development and test dataset had in each language pair respectively.} 
	\label{tab:hter_data}
\end{table}

\footnotetext{Language codes are available in ISO 639-1 Registration Authority Website Online - \url{ https://www.loc.gov/standards/iso639-2/php/code_list.php}}

\subsection{Predicting DA}
Even though HTER has been typically used to assess quality in machine translations, the reliability of this metric for assessing the performance of quality estimation systems has been questioned by researchers \cite{graham-etal-2016-glitters}. The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality \cite{graham_baldwin_moffat_zobel_2017}, where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics \cite{graham-etal-2015-accurate}. 

We used a recently created dataset to predict DA in machine translations which was released for the WMT 2020 quality estimation shared task 1 \cite{specia-etal-2020-findings-wmt}. The dataset is composed of data extracted from Wikipedia for six language pairs, consisting of high-resource English-German (En-De) and English-Chinese (En-Zh), medium-resource Romanian-English (Ro-En) and Estonian-English (Et-En), and low-resource Sinhala-English (Si-En) and Nepalese-English (Ne-En), as well as a Russian-English (En-Ru) dataset which combines articles from Wikipedia and Reddit \cite{fomicheva-etal-2020-unsupervised}. These datasets have been collected by translating sentences sampled from source-language articles using state-of-the-art NMT models built using the fairseq toolkit \cite{ott-etal-2019-fairseq} and annotated with DA scores by professional translators. Each translation was rated with a score from 0-100 according to the perceived translation quality by at least three translators \cite{specia-etal-2020-findings-wmt}. The DA scores were standardised using the z-score. The quality estimation systems evaluated on these datasets have to predict the mean DA z-scores of test sentence pairs. Each language pair has 7,000 sentence pairs in the training set, 1,000 sentence pairs in the development set and another 1,000 sentence pairs in the testing set. 

\section{Related Work}
\cite{kepler-etal-2019-openkiwi}
\section{STS for Translation Quality Estimation}

\section{Conclusion}

