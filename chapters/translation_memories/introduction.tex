\chapter{\label{cha:tm_introduction}Introduction to Translation Memories}

Translation Memories (TMs) are ``structured archives of past translations`` which store pairs of corresponding text segments\footnote{Segments are typically sentences, but there are implementations which consider longer or shorter units.} in source and target languages known as ``translation units'' \autocite{Simard2020}. TMs are used during the translation process in order to reuse previously translated segments. The original idea of TMs was proposed more than forty years ago when \autocite{Arthern1979} noticed that the translators working for the European Commission were wasting valuable time by re-translating (parts of) texts that had already been translated before. He proposed the creation of a computerised storage of source and target texts which could easily improve the performance of translators and that could be part of a computer-based terminology system. Based on this idea, many commercial TM systems appeared on the market in the early 1990s. Since then the use of this particular technology has kept growing and recent studies show that it is used on regular basis by a large proportion of translators  \autocite{zaretskaya:2018}.

TM systems help translators by continuously trying to provide them with so-called matches, which are translation proposals retrieved from its database. These matches are identified by comparing automatically the segment that has to be translated with all the segments stored in the database. There are three kinds of matches: exact, fuzzy and no matches. Exact matches are found if the segment to be translated is identical to one stored in the TM. Fuzzy matches are used in cases where it is possible to identify a segment which is similar enough to the one to be translated, and therefore, it is assumed that the translator will spend less time editing the translation retrieved from the database than translating the segment from scratch. No matches occur in cases where it is not possible to identify a fuzzy match (i.e. there is no segment similar enough to the one to be translated to be worth using its translation).

TMs distinguish between fuzzy matches and no matches by calculating the similarity between segments using a similarity measure and comparing it to a threshold. Most of the existing TM systems rely on a variant of the edit distance as the similarity measure and consider a fuzzy match when the edit distance score is between 70\% and 95\%.\footnote{It is unclear the origin for these value, but they are widely used by translators. Most of the tools allow translators to customise the value of this threshold according to their needs. Translators use their experience to decide which value for the threshold is appropriate for a given text.} The main justification for using this measure is the fact that edit distance can be easily calculated, is fast, and is largely language independent. However, edit distance is unable to capture the similarity between segments correctly when different wording and syntactic structures are used to express the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. To make this clearer, consider following three sentences. 

\begin{enumerate}
	\item I like Madrid which is such an attractive and exciting place.
	\item I dislike Madrid which is such an unattractive and unexciting place.
	\item I love Madrid as the city is full of attractions and excitements.
\end{enumerate}


Consider sentence 2 and 3 already had their translations in the TM database and now the sentence 1 was to be translated. Most of the commercial TM systems based on edit distance would return sentence 2 as a fuzzy match to the incoming sentence as the edit distance between sentence 1 and 2 are lower than sentences 1 and 3. However, sentences 1 and 3 are semantically very close and would not need a lot of edits in the post-editing process. This nature of the edit distance based TM systems of not proving semantically close matches can hinder the efficiency of the translators. 

Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section \ref{sec:tm_related} discusses some of the approaches proposed so far. Most of these approaches incorporate simple operations like paraphrasing to the TM matching process. As we noticed in Part I of the thesis, deep learning based architectures are the state-of-the-art in STS. Therefore, in Part II of the thesis we propose a novel TM matching and retrieval method based on deep learning which has the capability to capture semantically similar segments in TMs better than the methods based on edit distance. As we discussed in Part I of the thesis, in addition to providing state-of-the-art results, deep learning based STS methods can be adopted in other domains and languages easily which would be beneficial to TMs as TMs are employed in wide range of domains and languages. 

Utilising deep learning in TM matching methods would bring obvious challenges regarding efficiency and storage. In Chapter \ref{cha:tm_sentence_encoders}, we discuss those challenges and carefully pick STS methods that can be efficient in TM matching process. We evaluate those methods on a real-world TM comparing them with the edit distance. As far as we know, this is the first study done on employing a deep learning based STS metric in TM matching and retrieval. The main contributions of this part of the thesis are, 

\begin{enumerate}
	\item We perform a rigorous analysis on existing TM matching algorithms and identify the main shortcomings in them.
	\item We propose a novel TM matching and retrieval algorithm based on deep learning and evaluate it on a real-word TM using English-Spanish pairs. 
	\item We identify key challenges in employing deep learning in TM matching and we progressively develop a complete solution for the TM matching process using deep learning. 
\end{enumerate}

The remainder of this chapter is structured as follows. Section \ref{sec:tm_related} discusses the various TM matching algorithms and their short comings. In Section \ref{sec:tm_data}, we introduce the real-word TM we used for the experiments in this part of the thesis. Section \ref{sec:tm_eval} shows the evaluation metrics that we used to evaluate the experiments. Chapter concludes with the conclusions. 

\section{Related Work}
\label{sec:tm_related}
As we discussed before, even though TM systems have revolutionised the translation industry, these tools are far from being perfect. A serious shortcoming has to do with the fact that the (fuzzy) matching algorithm of most commercial TM systems is based on edit distance and no language processing is employed. Among the first ones to discuss the shortcomings were \autocite{10.5555/648180.749379} who maintained that Translation Memory technology was limited by the rudimentary techniques employed for approximate matching.  They comment that unless a TM system can perform morphological analysis, it will have difficulty recognising similar segments in the matching process.

The above shortcomings paved the way to the development of second-generation TM tools which had some language processing capabilities such as grammatical pattern recognition and performed limited segmentation at sub-sentence level. However, there are only a few commercially available second-generation TM systems such as \textit{Similis} \autocite{planas2005similis}, \textit{Translation Intelligence} \autocite{gronroos2005bringing} and Meta Morpho TM system, \textit{Morphologic} \autocite{hodasz2005metamorpho}. \textit{Similis} \autocite{planas2005similis} performs linguistic analysis in order to split sentences into syntactic chunks or syntagmas, making it easier for the system to retrieve matches. \textit{Morphologic} uses lemmas and part-of-speech information in order to improve matching, especially for morphologically rich languages like Hungarian \autocite{hodasz2005metamorpho}. Even though the second-generation TM tools solved some of the issues in first-generation TM tools, \autocite{mitkov2008improving} notice that they still can not provide strong matches in most of the cases. \autocite{mitkov2008improving} show that none of the second-generation TM systems would be capable of matching \textit{Microsoft developed Windows XP} with \textit{Windows XP was developed by Microsoft} or matching \textit{The company bought shares} with \textit{The company completed acquisition of shares}. 


To overcome this shortcoming \autocite{pekar2007new} developed the so-called third-generation TM tools which analyse the segments not only in terms of syntax but also in terms of semantics. \autocite{pekar2007new} perform linguistic processing over tree graphs \autocite{szpektor-etal-2004-scaling,10.1007/978-3-540-30586-6_1} followed by lexicosyntactic normalisation. Then similarity between syntactic-semantic tree graphs is computed and matches at sub-sentence level are established, using a similarity filter and a node distance filter. While this promising work was the first example of matching algorithms for future third-generation TM systems, the described approach was not deemed suitable for practical applications due to its very long processing time (it could take days to compare matches). Another method which performs matching at level of syntactic trees is proposed by \autocite{vanallemeersch2014improving}. The results presented in their paper are preliminary and the authors notice that tree matching method is ``prohibitively slow'' in this research too.

Further work towards the development of third-generation TM systems included paraphrasing and clause splitting. \autocite{raisa-timonera-mitkov-2015-improving} experimented with clause splitting and paraphrasing, seeking to establish whether these NLP tasks would improve the performance of TM systems in terms of matching. Furthermore in to this, \autocite{Gupta2016} experimented with paraphrasing the TM with a view to securing more matches. The authors sought to embed information from PPDB\footnote{PPDP is available on \url{http://paraphrase.org/\#/download}}, a database of paraphrases \autocite{ganitkevitch-etal-2013-ppdb}, in the edit distance metric by employing dynamic programming (DP) \autocite{Gupta2016}  as well as dynamic programming and greedy approximation (DPGA) \autocite{10.1007/978-3-319-45510-5_30}. In more recent work, \autocite{gupta-etal-2014-uow} developed a machine learning approach for semantic similarity and textual entailment based on features extracted using typed dependencies, paraphrasing, machine translation, evaluation metrics, quality estimation metrics and corpus pattern analysis. This similarity method was experimented with to retrieve the most similar segments from a translation memory but the evaluation results showed that the approach was too slow to be used in a real world scenario \autocite{gupta2014intelligent}. 

With this analysis, we identified two main limitations in current third-generation TM systems. First, most of them rely on external knowledge-bases like WordNet, PPDB which makes it difficult to use in many languages and domains. Secondly, most of these approaches are slow to be used in real-world applications. To address this we propose to use deep learning based STS metrics we experimented in Part I of the thesis in TM matching. As aforementioned, they are not dependent on external knowledge-bases and most of them have been optimised to use effectively in real-world scenarios. Therefore, in Chapter \ref{cha:tm_sentence_encoders} we evaluate those STS metrics in TM matching and retrieval. To best of our knowledge, deep learning methods have not been used successfully in translation memories. 


\section{Dataset}
\label{sec:tm_data}
For the experiments of this part of the Thesis we used the DGT-Translation Memory\footnote{DGT-TM is available to download at \url{https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memory}.} which has been made publicly available by the European Commission’s (EC) Directorate General for Translation (DGT) and the EC’s Joint Research Centre. It consists of sentences and their professional translations covering twenty-two official European Union (EU) languages and their 231 language pair combinations. DGT-TM contains official legal acts. The translations are produced by highly qualified human translators specialised in specific subject domains. It is typically used by translation professionals in combination with TM software to improve the speed and consistency of their translations. We should note that the DGT TM is a valuable resource for translation studies and for language technology applications, including statistical machine translation, terminology extraction, named entity recognition, multilingual classification and clustering, among others \autocite{aker-etal-2013-extracting, besacier-schwartz-2015-automated}. 

While we chose English-Spanish sentence pairs for the experiments of this study , our approach is easily extendable to any language pair. In this particular study, 2018 Volume 1 was used as experimental translation memory and 2018 Volume 3 was used as input sentences. The translation memory we built from 2018 Volume 1 featured 230,000 sentence pairs whilst, 2018 Volume 3 had 66,500 sentence pairs which we used as input sentences. 

\section{Evaluation}
\label{sec:tm_eval}
TM systems are usually evaluated by measuring the quality of the retrieved segment by the algorithm. Quality is considered to be the correspondence between retrieved segment and the reference translation: \textit{"the closer a retrieved segment is to a reference translation, the better it is"}. Scores are calculated for individual segments by comparing them with the relevant reference translations.  Those scores are then averaged over the whole corpus to reach an estimate of the quality of the TM system. These quality evaluating techniques are typically referred as automatic metrics for machine translation (MT) evaluation.

Over the years, researches have produced many automatic metrics for MT evaluations. BLEU (bilingual evaluation understudy) \autocite{papineni-etal-2002-bleu} can be considered as the most popular and oldest automatic metric. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most inexpensive metrics. However, using BLEU has drawbacks. The main drawback in BLEU is it does not consider meaning and does not directly consider sentence structure \autocite{sellam-etal-2020-bleurt}. Since the goal of this study is to provide TM matches that are closer in meaning, we did not consider BLEU as our evaluation metric. 

METEOR is a more recent automatic metric for MT evaluation which was designed to explicitly address several observed weaknesses in BLEU \autocite{banerjee-lavie-2005-meteor}. Similar to BLEU, METEOR is also based on an explicit word-to-word matching. However, unlike BLEU it does not only supports matching between words that are identical in the two strings being compared, but can also match words that are simple morphological variants of each other (i.e. they have an identical stem), and words that are synonyms of each other. Considering these advantages in using METEOR, we employed METEOR as our evaluation metric for the experiments in this part of the Thesis. 

It should be noted that the automatic evaluation metrics are far from being perfect \autocite{sellam-etal-2020-bleurt}. They have their own limitations which in turn can affect the evaluations of this study. Whatever the automatic evaluation metric we use, we would not be able to completely avoid these weaknesses. Therefore, in addition to the automatic evaluation, we also carried out a human evaluation. We asked three native Spanish speakers with a background in translation studies to compare the segments retrieved from our algorithm. In Chapter \ref{cha:tm_sentence_encoders}, we report these results too alongside the automatic evaluation metrics.

\section{Conclusions}
The Translation Memory (TM) tools revolutionised the work of professional translators and the last three decades have seen dramatic changes in the translation workflow. One of the most important functions of TM systems is its ability to match a sentence to be translated against the database. However, most of the current commercial TM systems rely on edit distance to provide TM matches. Despite being simple, edit distance is unable to capture the similarity between segments and as a result even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it. This can hinder the performance of translators who are using the TM. 

As a solution to this second-generation and third-generation TM systems are proposed. However, they are far from being perfect. Most of them lack the efficiency which is required for TM systems. Furthermore they rely on language specific knowledge-bases which makes them less adoptable to other languages and domains. Therefore, to overcome these shortcomings, we propose a novel TM matching and retrieval algorithm based on STS methods we experimented in Part I of the thesis. In addition to providing state-of-the-art STS results these algorithms are fast and easily adoptable to other languages and domain which would be beneficial for TMs. 

We will be using English-Spanish sentence pairs in DGT translation memory as the dataset for our experiments. Our evaluation will be based on METEOR; an automatic metric for MT evaluation. Furthermore, considering the limitations in automatic metrics, we would be incorporating a human evaluation too in our experiments. The proposed method, results and evaluation will be explained in-detail in Chapter \ref{cha:tm_sentence_encoders}.







