\chapter{\label{cha:tm_introduction}Introduction to Translation Memories}

Translation Memories (TMs) are ``structured archives of past translations'' which store pairs of corresponding text segments\footnote{Segments are typically sentences, but there are implementations which consider longer or shorter units.} in source and target languages known as ``translation units'' \autocite{Simard2020}. TMs are used during the translation process in order to reuse previously translated segments. The original idea of TMs was proposed more than forty years ago when \textcite{Arthern1979} noticed that the translators working for the European Commission were wasting valuable time by re-translating (parts of) texts that had already been translated before. He proposed the creation of a computerised storage of source and target texts which could easily improve the translators' performance. This storage could be part of a computer-based terminology system. Based on this idea, many commercial TM systems appeared on the market in the early 1990s \autocite{bowker2006lexicography}. Since then, the use of this particular technology has kept growing, and recent studies show that it is used on a regular basis by a large proportion of translators  \autocite{zaretskaya:2018}.

TM systems help translators by continuously providing them with so-called matches, which are translation proposals retrieved from its database. These matches are identified automatically by comparing the segment to be translated with all of the segments stored in the database. There are three kinds of matches: exact, fuzzy and no matches. Exact matches refer to when the segment to be translated is identical to one stored in the TM. Fuzzy matches are used in cases where it is possible to identify a similar segment to the one to be translated. Therefore, it is assumed that the translator will spend less time editing the translation retrieved from the database than translating the segment from scratch. No matches occur when it is impossible to identify a fuzzy match (i.e. there is no segment similar enough to the one to be translated).

TMs distinguish between fuzzy and no matches by calculating the similarity between segments using a similarity measure and comparing it to a threshold. Most of the existing TM systems rely on a variant of the edit distance as the similarity measure and consider a fuzzy match when the edit distance score is between 70\% and 95\%\footnote{It is unclear the origin for these values, but translators widely use them. Most of the tools allow translators to customise the value of this threshold according to their needs. Translators use their experience to decide which value for the threshold is appropriate for a given text.}. The main justification for using this measure is the fact that the edit distance between two texts can easily be calculated, it is fast and it is largely language-independent. However, edit distance cannot capture the similarity between segments correctly when different wording and syntactic structures are used to express the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not identify it in most cases. To make this clearer, consider the following three sentences. 
 

\begin{enumerate}
	\item I like Madrid which is such an attractive and exciting place.
	\item I dislike Madrid which is such an unattractive and unexciting place.
	\item I love Madrid as the city is full of attractions and excitements.
\end{enumerate}


Assume sentences 2 and 3 already had their translations in the TM database, and now sentence 1 has to be translated. The majority of the commercial TM systems based on edit distance return sentence 2 as a fuzzy match to the incoming sentence since the edit distance between sentences 1 and 2 are lower than sentences 1 and 3. However, sentence 3 is semantically closer to sentence 1 than sentence 2 and does not need many edits in the post-editing process. This imperfections of the edit distance based TM systems of not providing semantically close matches hinders the translators' efficiency \autocite{ranasinghe-etal-2020-intelligent}. 

Researchers address this shortcoming of the edit distance metric by employing similarity metrics to identify semantically similar segments even when they are different at the token level. Section \ref{sec:tm_related} discusses some of the approaches proposed so far. These approaches incorporate simple operations like paraphrasing to the TM matching process to provide semantically similar matches. As we observed in Part I of the thesis, deep learning based architectures are state-of-the-art in STS. Therefore, in Part II of the thesis, we propose a novel TM matching and retrieval method based on deep learning that can capture semantically similar segments in TMs better than the methods based on edit distance. As we discussed in Part I of the thesis, in addition to providing state-of-the-art results, deep learning based STS methods can easily be adapted to other languages and domains, which is beneficial for TMs as they are employed in a wide range of domains and languages. 

Utilising deep learning in TM matching methods bring obvious challenges regarding efficiency and storage. In Chapter \ref{cha:tm_sentence_encoders}, we discuss these challenges and carefully pick STS methods that are efficient in the TM matching process. We evaluate these methods on a real-world TM, comparing them with the edit distance. As far as we know, this is the first study done on employing deep learning based STS methods in TM matching and retrieval. The main contributions of this part of the thesis are, 

\begin{enumerate}
	\item We perform a rigorous analysis of existing TM matching algorithms and identify their main shortcomings.
	\item We propose a novel TM matching and retrieval algorithm based on deep learning and evaluate it on a real-world TM using English-Spanish pairs. 
	\item We compare the results of the proposed method with an existing TM system and show that our approach improves the TM matching and retrieving process.
\end{enumerate}

The remainder of this chapter is structured as follows. Section \ref{sec:tm_related} discusses the various TM matching algorithms and their shortcomings. In Section \ref{sec:tm_data}, we introduce the real-word TM we used for the experiments in this part of the thesis. Section \ref{sec:tm_eval} shows the evaluation metrics that we used to evaluate the experiments. The chapter finishes with the conclusions. 

\section{Related Work}
\label{sec:tm_related}
As discussed before, even though TM systems have revolutionised the translation industry, these tools are far from perfect. A serious shortcoming is that most commercial TM systems' (fuzzy) matching algorithm is based on edit distance, and and does not employ any language processing. Among the first ones to discuss the shortcomings were \textcite{10.5555/648180.749379} who showed that Translation Memory technology was limited by the rudimentary techniques employed for approximate matching.  They comment that unless a TM system can perform morphological analysis, it will have difficulty recognising similar segments in the matching process.

The above shortcomings paved the way for developing second-generation TM tools, which had some language processing capabilities such as grammatical pattern recognition and performed limited segmentation at the sub-sentence level. However, there are only a few commercially available second-generation TM systems such as \textit{Similis} \autocite{planas2005similis}, \textit{Translation Intelligence} \autocite{gronroos2005bringing} and Meta Morpho TM system, \textit{Morphologic} \autocite{hodasz2005metamorpho}. \textit{Similis} \autocite{planas2005similis} performs linguistic analysis to split sentences into syntactic chunks or syntagmas, making it easier for the system to retrieve matches. \textit{Morphologic} uses lemmas and part-of-speech information to improve matching, especially for morphologically rich languages such as Hungarian \autocite{hodasz2005metamorpho}. Even though the second-generation TM tools solved some of the issues in first-generation TM tools, \textcite{mitkov2008improving} discuss that they still can not provide strong matches in most of the cases. \textcite{mitkov2008improving} show that none of the second-generation TM systems would be capable of matching \textit{Microsoft developed Windows XP} with \textit{Windows XP was developed by Microsoft} or matching \textit{The company bought shares} with \textit{The company completed the acquisition of shares}. 


To overcome this shortcoming, \textcite{pekar2007new} developed the so-called third-generation TM tools, which analyse the segments not only in terms of syntax but also in terms of semantics. \textcite{pekar2007new} perform linguistic processing over tree graphs \autocite{szpektor-etal-2004-scaling,10.1007/978-3-540-30586-6_1} followed by lexicosyntactic normalisation. Then similarity between syntactic-semantic tree graphs is computed, and matches at the sub-sentence level are established using a similarity filter and a node distance filter. While this promising work was the first example of matching algorithms for future third-generation TM systems, the described approach was not deemed suitable for practical applications due to its very long processing time (it could take days to compare matches). Another method that performs matching at the level of syntactic trees was proposed by \textcite{vanallemeersch2014improving}. The results presented in their paper are preliminary, and the authors note that the tree matching method is ``prohibitively slow''.

Further work towards the development of third-generation TM systems included paraphrasing and clause splitting. \textcite{raisa-timonera-mitkov-2015-improving} experimented with clause splitting and paraphrasing, seeking to establish whether these NLP tasks can improve the performance of TM systems in terms of matching. Furthermore, \textcite{Gupta2016} experimented with incorporating paraphrasing to the TM matching algorithm to secure more matches. The authors sought to embed information from PPDB\footnote{PPDP is available on \url{http://paraphrase.org/\#/download}}, a database of paraphrases \autocite{ganitkevitch-etal-2013-ppdb}, in to the edit distance metric by employing dynamic programming (DP) \autocite{Gupta2016}  as well as dynamic programming and greedy approximation (DPGA) \autocite{10.1007/978-3-319-45510-5_30}. In more recent work, \textcite{gupta-etal-2014-uow} developed a machine learning approach for semantic similarity and textual entailment based on features extracted using typed dependencies, paraphrasing, machine translation, evaluation metrics, quality estimation metrics and corpus pattern analysis. This similarity method was experimented with to retrieve the most similar segments from a translation memory. The evaluation results showed that this approach was too slow to be used in a real-world scenario \autocite{gupta2014intelligent}. 

With this analysis, we identified two key limitations in the current third-generation TM systems. Firstly, most of them rely on external knowledge bases, including WordNet and PPDB, which are challenging to use in many languages and domains. Secondly, the majority of these approaches are too slow to be used in real-world applications. To address these limitations, we propose using the deep learning based STS metrics we experimented in Part I of the thesis in TM matching. As aforementioned, these methods do not depend on external knowledge bases, and most of them are optimised to be used effectively in real-world scenarios. Therefore, in Chapter \ref{cha:tm_sentence_encoders} we evaluate these STS metrics in TM matching and retrieval. To the best of our knowledge, this is the first study to employ deep learning in translation memories. 




\section{Dataset}
\label{sec:tm_data}
For the experiments in this part of the thesis, we used the DGT-Translation Memory\footnote{DGT-TM is available to download at \url{https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memory}.} which has been made publicly available by the European Commissionâ€™s (EC) Directorate General for Translation (DGT), and the EC Joint Research Centre. DGT-TM contains official legal acts. It consists of sentences and their professional translations covering twenty-two official European Union (EU) languages and their 231 language pair combinations. The translations are produced by highly qualified human translators specialised in specific subject domains. DGT TM is typically used by translation professionals in combination with TM software to improve the speed and consistency of their translations. We should note that the DGT TM is a valuable resource for translation studies and for language technology applications, including statistical machine translation, terminology extraction, named entity recognition, multilingual classification and clustering, among others \autocite{aker-etal-2013-extracting, besacier-schwartz-2015-automated}. 

While we chose English-Spanish sentence pairs for the experiments of this study, our approach is easily extendable to any language pair. In this study, 2018 Volume 1 was used as the experimental translation memory and 2018 Volume 3 as input sentences. The translation memory we built from 2018 Volume 1 featured 230,000 sentence pairs whilst 2018 Volume 3 had 66,500 sentence pairs which we used as input sentences. 

\section{Evaluation}
\label{sec:tm_eval}
TM systems are typically evaluated by measuring the \textit{quality} of the retrieved segments from the matching algorithm \autocite{gupta-etal-2015-translation}. This \textit{quality} is often considered to be the correspondence between the retrieved segment and the reference translation: \textit{"the closer a retrieved segment is to a reference translation, the better it is"}. First, the quality scores are calculated for individual segments by comparing them with the relevant reference translations.  These scores are then averaged over the whole corpus to estimate the quality of the TM system. Such quality evaluation techniques between the retrieved segment and the reference translation are called automatic metrics for machine translation evaluation.

Over the years, researchers have produced many automatic metrics for MT evaluations. BLEU (bilingual evaluation understudy) \autocite{papineni-etal-2002-bleu} is the oldest and most popular automatic metric. BLEU was one of the first metrics to claim a high correlation with human judgements of quality and remains one of the most inexpensive metrics \autocite{gupta-etal-2015-machine}. However, using BLEU has drawbacks. The main drawbacks of BLEU is it does not consider the meaning and, does not directly consider sentence structure \autocite{sellam-etal-2020-bleurt}. Since this study aims to provide TM matches that are closer in meaning, we did not consider BLEU as our evaluation metric. 

METEOR is a more recent automatic metric for MT evaluation that was designed to explicitly address several observed weaknesses in BLEU \autocite{banerjee-lavie-2005-meteor}. Like BLEU, METEOR is also based on explicit word-to-word matching. However, unlike BLEU, it not only supports matching between identical words in the two strings compared, but can also match words that are simple morphological variants of each other (i.e. they have an identical stem), and words that are synonyms of each other. Considering these advantages, we employed METEOR as our evaluation metric for the experiments in this part of the thesis. 

It should be noted that the automatic evaluation metrics are far from perfect \autocite{sellam-etal-2020-bleurt}. These metrics have their own limitations, which can affect the evaluations of this study. Whatever automatic evaluation metric we use, we would not be able to avoid these weaknesses completely. Therefore, in addition to the automatic evaluation, we carried out a human evaluation. We asked three native Spanish speakers with a background in translation studies to compare the segments retrieved from our algorithm. In Chapter \ref{cha:tm_sentence_encoders}, we report these results alongside the automatic evaluation metrics.


\section{Conclusions}
The Translation Memory (TM) tools revolutionised the work of professional translators, and the last three decades have shown dramatic changes in the translation workflow. One of the essential functions of TM systems is their ability to match a sentence to be translated against the database. However, most of the current commercial TM systems rely on edit distance to provide TM matches. Despite being simple, edit distance is unable to capture the similarity between segments. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it. This can hinder the performance of translators who are using the TM. 

Second-generation and third generation TM systems were proposed to address this limitation. However, they are far from being perfect. Most of them lack the efficiency which is required for TM systems. Furthermore, they rely on language-specific knowledge bases, which makes them less adaptable to other languages and domains. To overcome these shortcomings, we propose a novel TM matching and retrieval algorithm based on STS methods as we experimented with in Part I of the thesis. In addition to providing state-of-the-art STS results, these algorithms are fast and easily adaptable to other languages and domains, which is beneficial for TMs. 

We will be using English-Spanish sentence pairs in DGT translation memory as the dataset for our experiments. Our evaluation will be based on METEOR, an automatic metric for MT evaluation. Furthermore, considering the limitations in automatic metrics, we will also incorporate a human evaluation in our experiments. The proposed method, results and evaluation will be explained in detail in Chapter \ref{cha:tm_sentence_encoders}.







