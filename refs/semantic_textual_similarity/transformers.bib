@inproceedings{yang2019xlnet,
	title={Xlnet: Generalized autoregressive pretraining for language understanding},
	author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	booktitle={Advances in neural information processing systems},
	pages={5753--5763},
	year={2019}
}

@inproceedings{Lan2020ALBERT,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@inproceedings{Clark2020ELECTRA,
	title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
	author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=r1xMH1BtvB}
}

@article{JMLR:v17:16-272,
	author  = {Benigno Uria and Marc-Alexandre C{{\^o}}t{{\'e}} and Karol Gregor and Iain Murray and Hugo Larochelle},
	title   = {Neural Autoregressive Distribution Estimation},
	journal = {Journal of Machine Learning Research},
	year    = {2016},
	volume  = {17},
	number  = {205},
	pages   = {1-37},
	url     = {http://jmlr.org/papers/v17/16-272.html}
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186"
}

@article{liu2019roberta,
	title={{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}

@article{joshi-etal-2020-spanbert,
	title = "{S}pan{BERT}: Improving Pre-training by Representing and Predicting Spans",
	author = "Joshi, Mandar  and
	Chen, Danqi  and
	Liu, Yinhan  and
	Weld, Daniel S.  and
	Zettlemoyer, Luke  and
	Levy, Omer",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "8",
	year = "2020",
	url = "https://www.aclweb.org/anthology/2020.tacl-1.5",
	doi = "10.1162/tacl_a_00300",
	pages = "64--77"
}

@article{zaheer2021,
	title={Big Bird: Transformers for Longer Sequences}, 
	author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
	journal={arXiv preprint arXiv:2007.14062},
	year={2021}
}

@article{beltagy2020,
	title={Longformer: The Long-Document Transformer}, 
	author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
	journal={arXiv preprint arXiv:2004.05150},
	year={2020}
}

